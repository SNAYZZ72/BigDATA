-- Initialisation
cd vagrant-projects/OracleDatabase/21.3.0
vagrant up
vagrant ssh
start-dfs.sh
start-yarn.sh
export PROJECTHOME=../../vagrant/projet
java -jar $KVHOME/lib/kvstore.jar kvlite -root $KVROOT -host localhost &
nohup hive --service hiveserver2 &

-- KVSTORE
java -jar $KVHOME/lib/kvstore.jar runadmin -port 5000 -host localhost
connect store -name kvstore
execute 'drop table marketing';

execute "create table IF NOT EXISTS marketing (
    age number, 
    sexe string, 
    taux number, 
    situationFamiliale string, 
    nbEnfantsAcharge number, 
    deuxiemeVoiture boolean, 
    id number generated by default as identity, 
    primary key (id))";

javac -g -cp $KVHOME/lib/kvclient.jar:$PROJECTHOME/data $PROJECTHOME/program/marketing/MarketingImportData.java
java -Xmx256m -Xms256m -cp $KVHOME/lib/kvclient.jar:$PROJECTHOME/program marketing.MarketingImportData
get table -name MARKETING

-- MONGODB
mongoimport --host localhost --port 27017 --db projet --collection immatriculations --type csv --headerline --file "/vagrant/projet/data/Immatriculations.csv"
mongo
use projet
db.immatriculations.find()
db.immatriculations.drop()

-- HDFS
hadoop fs -rm -r /user/db
hadoop fs -mkdir /user/db

hadoop fs -mkdir /user/db/Catalogue
hadoop fs -put $PROJECTHOME/data/Catalogue.csv /user/db/Catalogue
hadoop fs -ls /user/db/Catalogue

hadoop fs -mkdir /user/db/Clients_7
hadoop fs -put $PROJECTHOME/data/Clients_7.csv /user/db/Clients_7
hadoop fs -ls /user/db/Clients_7

hadoop fs -mkdir /user/db/Clients_12
hadoop fs -put $PROJECTHOME/data/Clients_12.csv /user/db/Clients_12
hadoop fs -ls /user/db/Clients_12

-- HIVE
beeline
!connect jdbc:hive2://localhost:10000
Enter username for jdbc:hive2://localhost:10000: oracle
Enter password for jdbc:hive2://localhost:10000: welcome1

drop table IMMATRICULATION_EXT;

CREATE EXTERNAL TABLE  IMMATRICULATION_EXT  (
    IMMATRICULATION string, 
    MARQUE string, NOM string, 
    PUISSANCE string, 
    LONGUEUR string, 
    NBPLACES string, 
    NBPORTES string, 
    COULEUR string, 
    OCCASION string, 
    PRIX string) 
    STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler' 
    WITH SERDEPROPERTIES('mongo.columns.mapping'='{"id":"_id", "nbPlaces":"nbPlaces", "nbPortes":"nbPortes"}') 
    TBLPROPERTIES ('mongo.uri'='mongodb://localhost:27017/projet.immatriculations');

select * from IMMATRICULATION_EXT limit 5;

drop table MARKETING_EXT;

CREATE EXTERNAL TABLE  MARKETING_EXT  (
    ID int, 
    AGE string, 
    SEXE string, 
    TAUX string, 
    SITUATIONFAMILIALE string, 
    NBENFANTSACHARGE string, 
    DEUXIEMEVOITURE string) 
    STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler' 
    TBLPROPERTIES ("oracle.kv.kvstore" = "kvstore", 
    "oracle.kv.hosts" = "localhost:5000",  
    "oracle.kv.hadoop.hosts" = "localhost/127.0.0.1",  
    "oracle.kv.tableName" = "MARKETING");

select * from MARKETING_EXT limit 5;

drop table CATALOGUE_EXT;

CREATE EXTERNAL TABLE  CATALOGUE_EXT  (
    MARQUE string, 
    NOM string, 
    PUISSANCE string, 
    LONGUEUR string, 
    NBPLACES string, 
    NBPORTES string, 
    COULEUR string, 
    OCCASION string, 
    PRIX string) 
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
    STORED AS TEXTFILE LOCATION 'hdfs:/user/db/Catalogue'
    TBLPROPERTIES ("skip.header.line.count"="1");

select * from CATALOGUE_EXT limit 5;

drop table CLIENTS_7_EXT;

CREATE EXTERNAL TABLE  CLIENTS_7_EXT  (
    AGE string, 
    SEXE string, 
    TAUX string, 
    SITUATIONFAMILIALE string, 
    NBENFANTSACHARGE string, 
    DEUXIEMEVOITURE string, 
    IMMATRICULATION string) 
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
    STORED AS TEXTFILE LOCATION 'hdfs:/user/db/Clients_7'
    TBLPROPERTIES ("skip.header.line.count"="1");

select * from CLIENTS_7_EXT limit 5;

drop table CLIENTS_12_EXT;

CREATE EXTERNAL TABLE  CLIENTS_12_EXT  (
    AGE string, 
    SEXE string, 
    TAUX string, 
    SITUATIONFAMILIALE string, 
    NBENFANTSACHARGE string, 
    DEUXIEMEVOITURE string, 
    IMMATRICULATION string) 
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
    STORED AS TEXTFILE LOCATION 'hdfs:/user/db/Clients_12'
    TBLPROPERTIES ("skip.header.line.count"="1");

select * from CLIENTS_12_EXT limit 5;

-- SQLPLUS
sudo su oracle
sqlplus SARMENTOBZ2324/SARMENTOBZ232401@129.151.234.184:1521/pdbestia.sub12051533510.vcnmiageuca.oraclevcn.com

create or replace directory ORACLE_BIGDATA_CONFIG as '/vagrant/bigdatasql_config';
create or replace directory "ORA_BIGDATA_CL_bigdatalite" as '';

select DIRECTORY_NAME, DIRECTORY_PATH from dba_directories;
SELECT table_name FROM user_tables;

drop table IMMATRICULATION;

CREATE TABLE IMMATRICULATION (
    IMMATRICULATION VARCHAR2(255), 
    MARQUE VARCHAR2(255), 
    NOM VARCHAR2(255), 
    PUISSANCE VARCHAR2(255), 
    LONGUEUR VARCHAR2(255), 
    NBPLACES VARCHAR2(255), 
    NBPORTES VARCHAR2(255), 
    COULEUR VARCHAR2(255), 
    OCCASION VARCHAR2(255), 
    PRIX VARCHAR2(255)
    ) 
    ORGANIZATION EXTERNAL (
        TYPE ORACLE_HIVE
        DEFAULT DIRECTORY ORACLE_BIGDATA_CONFIG 
        ACCESS PARAMETERS (
            com.oracle.bigdata.tablename=default.IMMATRICULATION_EXT
        )
    )
    REJECT LIMIT UNLIMITED;

SELECT * FROM IMMATRICULATION;


-- Connexion entre Hive et R
sudo su
cd $PROJECTHOME/program/R

R CMD javareconf
options(repos = "https://cran.rstudio.com/")
install.packages("RJDBC")

Rscript connexion_hive.R

-- Hadoop mapreduce
hadoop fs -mkdir /user/db/CO2
hadoop fs -put $PROJECTHOME/data/CO2.csv /user/db/CO2
hadoop fs -ls /user/db/CO2

hadoop fs -rm /user/db/CO2_output/*                                                                                                                         */
hadoop fs -rmdir /user/db/CO2_output
hadoop fs -mkdir /user/db/CO2_output
hadoop fs -ls /user/db/CO2_output

hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \
  -files /vagrant/projet/program/mapreduce/Mapper.py,/vagrant/projet/program/mapreduce/Reducer.py \
  -mapper 'python3 /vagrant/projet/program/mapreduce/Mapper.py' \
  -reducer 'python3 /vagrant/projet/program/mapreduce/Reducer.py' \
  -input /user/db/CO2 \
  -output /user/db/CO2_output

hadoop fs -cat /user/db/CO2_output/part-00000

hadoop fs -get /user/db/CO2_output/part-00000 /vagrant/projet/program/mapreduce/CO2_output.txt

cd /vagrant/projet/program/mapreduce
sed 's/\t/,/g' CO2_output.txt > CO2_output.csv
sed -i '1s/^/marque,bonusMalus,rejet,coutEnergie\n/' CO2_output.csv
cp CO2_output.csv /vagrant/projet/data

cd /vagrant/projet/program/R
Rscript merge_catalogue.R